import express from 'express';
import cors from 'cors';
import { fileURLToPath } from 'url';
import { dirname, join } from 'path';
import { existsSync } from 'fs';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const app = express();
const port = process.env.PORT || 4000;

// Middleware
app.use(cors({
  origin: ['http://localhost:3000', 'https://tu-portfolio.com'],
  credentials: true
}));
app.use(express.json());

// Variable global para el modelo (se cargarÃ¡ cuando estÃ© disponible)
let llama = null;
let isModelLoading = false;
let modelError = null;

// FunciÃ³n para cargar el modelo (simulada por ahora)
async function loadModel() {
  if (isModelLoading || llama) return;
  
  isModelLoading = true;
  console.log('ğŸ”„ Intentando cargar modelo LLM...');
  
  try {
    // Por ahora simulamos la carga del modelo
    // En una implementaciÃ³n real, aquÃ­ cargarÃ­as node-llama-cpp
    /*
    const { LlamaCpp } = await import('node-llama-cpp');
    const modelPath = join(__dirname, 'models', 'llama-7b-q4_0.gguf');
    
    if (!existsSync(modelPath)) {
      throw new Error(`Modelo no encontrado en: ${modelPath}`);
    }
    
    llama = new LlamaCpp({
      modelPath,
      contextSize: 2048,
      seed: 42
    });
    
    await llama.init();
    */
    
    // SimulaciÃ³n de carga exitosa
    await new Promise(resolve => setTimeout(resolve, 2000));
    llama = { ready: true }; // Mock del modelo
    
    console.log('âœ… Modelo LLM cargado exitosamente');
  } catch (error) {
    console.error('âŒ Error cargando modelo:', error.message);
    modelError = error.message;
  } finally {
    isModelLoading = false;
  }
}

// FunciÃ³n para generar respuesta (simulada)
async function generateResponse(history, res) {
  console.log('ğŸ¤– [Backend] Generating response for history:', history.length, 'messages');
  
  // Obtener el Ãºltimo mensaje del usuario
  const lastUserMessage = history.filter(msg => msg.role === 'user').pop();
  const userText = lastUserMessage?.content || '';
  console.log('ğŸ’¬ [Backend] User message:', userText);
  
  // Obtener el system prompt para entender el contexto
  const systemMessage = history.find(msg => msg.role === 'system');
  const systemPrompt = systemMessage?.content || '';
  
  // Simulamos una respuesta mÃ¡s inteligente de Clippy
  let response = generateClippyResponse(userText, history, systemPrompt);
  
  function generateClippyResponse(userText, conversationHistory, systemPrompt) {
    const text = userText.toLowerCase();
    
    // Respuestas de saludo
    if (text.includes('hola') || text.includes('hello') || text.includes('hi ')) {
      const greetings = [
        'Â¡Hola! Soy Clippy, tu asistente de oficina favorito. Â¿En quÃ© puedo ayudarte hoy?',
        'Â¡Hola! Me alegra verte de nuevo. Â¿Hay algo en lo que pueda asistirte?',
        'Â¡Saludos! Soy Clippy y estoy aquÃ­ para hacer tu trabajo mÃ¡s fÃ¡cil. Â¿QuÃ© necesitas?'
      ];
      return greetings[Math.floor(Math.random() * greetings.length)];
    }
    
    // Preguntas sobre identidad
    if (text.includes('quien eres') || text.includes('who are you') || text.includes('quÃ© eres')) {
      return 'Soy Clippy, el asistente de Microsoft Office que ha estado ayudando a usuarios desde 1997. Aunque originalmente ayudaba con documentos de Word, ahora puedo asistirte con una gran variedad de tareas. Â¡Siempre estoy listo para ayudar!';
    }
    
    // Preguntas sobre capacidades
    if (text.includes('quÃ© puedes hacer') || text.includes('what can you do') || text.includes('ayuda')) {
      return 'Puedo ayudarte con muchas cosas: responder preguntas, explicar conceptos, ayudarte con tareas de oficina, resolver problemas, dar consejos, y mucho mÃ¡s. Mi objetivo es ser tu asistente Ãºtil y amigable. Â¿Hay algo especÃ­fico en lo que te gustarÃ­a que te ayude?';
    }
    
    // Preguntas tÃ©cnicas o de programaciÃ³n
    if (text.includes('cÃ³digo') || text.includes('programar') || text.includes('javascript') || text.includes('python') || text.includes('html')) {
      return 'Me encanta ayudar con programaciÃ³n. Aunque mi especialidad original era Microsoft Office, he aprendido mucho sobre desarrollo web y programaciÃ³n. Â¿QuÃ© tipo de cÃ³digo estÃ¡s escribiendo? Puedo ayudarte con sintaxis, debugging, mejores prÃ¡cticas, o explicarte conceptos.';
    }
    
    // Preguntas sobre trabajo/productividad
    if (text.includes('trabajo') || text.includes('productividad') || text.includes('organizar') || text.includes('planificar')) {
      return 'La productividad es mi especialidad. Puedo ayudarte a organizar tareas, crear planes de trabajo, optimizar procesos, o darte consejos para ser mÃ¡s eficiente. Â¿En quÃ© Ã¡rea especÃ­fica te gustarÃ­a mejorar tu productividad?';
    }
    
    // Respuestas emocionales/motivacionales
    if (text.includes('cansado') || text.includes('estresado') || text.includes('difÃ­cil') || text.includes('problema')) {
      return 'Entiendo que a veces las cosas pueden ser desafiantes. Como tu asistente, estoy aquÃ­ para ayudarte a encontrar soluciones y hacer las cosas mÃ¡s fÃ¡ciles. Â¿Puedes contarme mÃ¡s sobre lo que te estÃ¡ causando dificultades? Juntos podemos encontrar una manera de resolverlo.';
    }
    
    // Preguntas sobre el futuro o tecnologÃ­a
    if (text.includes('futuro') || text.includes('ia') || text.includes('inteligencia artificial') || text.includes('tecnologÃ­a')) {
      return 'La tecnologÃ­a ha avanzado mucho desde mis primeros dÃ­as en Office 97. Es emocionante ver cÃ³mo la IA y las nuevas tecnologÃ­as estÃ¡n cambiando la forma en que trabajamos. Aunque soy un asistente clÃ¡sico, me adapto a los nuevos tiempos para seguir siendo Ãºtil. Â¿Te interesa algÃºn aspecto particular de la tecnologÃ­a?';
    }
    
    // Respuesta por defecto mÃ¡s inteligente
    const userWords = userText.split(' ').filter(word => word.length > 3);
    const keyWords = userWords.slice(0, 3).join(', ');
    
    const defaultResponses = [
      `Interesante pregunta sobre ${keyWords}. Como Clippy, siempre trato de ser Ãºtil. Â¿PodrÃ­as darme un poco mÃ¡s de contexto para poder ayudarte mejor?`,
      `Veo que mencionas ${keyWords}. Me gustarÃ­a ayudarte con eso. Â¿Puedes explicarme mÃ¡s detalles sobre lo que necesitas?`,
      `Entiendo tu consulta sobre ${keyWords}. Estoy aquÃ­ para asistirte. Â¿Hay algÃºn aspecto especÃ­fico en el que te gustarÃ­a que me enfoque?`,
      `Gracias por compartir eso sobre ${keyWords}. Como tu asistente, quiero asegurarme de darte la mejor ayuda posible. Â¿QuÃ© tipo de asistencia estÃ¡s buscando exactamente?`
    ];
    
    return defaultResponses[Math.floor(Math.random() * defaultResponses.length)];
  }
  
  console.log('ğŸ“ [Backend] Generated response:', response);
  const words = response.split(' ');
  console.log('ğŸ”¤ [Backend] Splitting into', words.length, 'tokens');
  
  // Enviamos cada palabra como un token separado
  for (let i = 0; i < words.length; i++) {
    const token = i === words.length - 1 ? words[i] : words[i] + ' ';
    
    console.log('ğŸ“¤ [Backend] Sending token:', token);
    // Enviamos el token via SSE
    res.write(`data: ${JSON.stringify({ token })}\n\n`);
    
    // PequeÃ±a pausa para simular el streaming
    await new Promise(resolve => setTimeout(resolve, 100 + Math.random() * 200));
  }
  
  console.log('âœ… [Backend] Response completed, sending done event');
  // SeÃ±al de finalizaciÃ³n
  res.write('event: done\ndata: {}\n\n');
}

// Endpoint principal de chat con streaming
app.post('/api/chat', async (req, res) => {
  console.log('ğŸš€ [Backend] Received chat request');
  
  try {
    const { modelAlias, systemPrompt, initialPrompts, topK, temperature } = req.body;
    console.log('ğŸ“‹ [Backend] Request body:', { 
      modelAlias, 
      systemPrompt: systemPrompt?.substring(0, 50) + '...', 
      initialPromptsLength: initialPrompts?.length,
      topK,
      temperature
    });
    
    if (!modelAlias || !systemPrompt || !Array.isArray(initialPrompts)) {
      console.error('âŒ [Backend] Invalid LlmOptions format');
      return res.status(400).json({ error: 'Se requieren modelAlias, systemPrompt e initialPrompts' });
    }
    
    // Construir el historial completo incluyendo el system prompt
    const history = [
      { role: 'system', content: systemPrompt },
      ...initialPrompts
    ];
    
    // Configurar headers para Server-Sent Events
    console.log('ğŸ“¡ [Backend] Setting SSE headers');
    res.set({
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Headers': 'Cache-Control'
    });
    
    // Verificar si el modelo estÃ¡ disponible (simulado)
    if (!llama && !isModelLoading) {
      console.log('ğŸ”„ [Backend] Starting model loading simulation');
      loadModel(); // Iniciar carga si no estÃ¡ en proceso
    }
    
    // Para la simulaciÃ³n, asumimos que el modelo estÃ¡ siempre disponible
    console.log('âœ… [Backend] Model ready, starting response generation');
    
    // Generar y enviar respuesta
    await generateResponse(history, res);
    
    console.log('ğŸ [Backend] Response stream completed');
    res.end();
    
  } catch (error) {
    console.error('âŒ [Backend] Error in /api/chat:', error);
    
    if (!res.headersSent) {
      console.log('ğŸ“¤ [Backend] Sending JSON error response');
      res.status(500).json({ error: error.message });
    } else {
      console.log('ğŸ“¤ [Backend] Sending SSE error event');
      res.write(`event: error\ndata: ${JSON.stringify({ message: error.message })}\n\n`);
      res.end();
    }
  }
});

// Endpoint de estado del modelo
app.get('/api/model/status', (req, res) => {
  res.json({
    loaded: !!llama,
    loading: isModelLoading,
    error: modelError
  });
});

// Endpoints para compatibilidad con el frontend
app.get('/api/state', (req, res) => {
  res.json({
    settings: {
      selectedModel: 'clippy-llm',
      systemPrompt: 'Eres Clippy, el asistente amigable de Microsoft Office.',
      disableAutoUpdate: false
    },
    models: {
      'clippy-llm': {
        name: 'Clippy LLM',
        size: 1000000,
        company: 'Local',
        description: 'Modelo local de Clippy',
        downloaded: true,
        imported: true
      }
    }
  });
});

app.post('/api/state/:key', (req, res) => {
  // Simular actualizaciÃ³n de estado
  res.json({ success: true });
});

app.get('/api/debug', (req, res) => {
  res.json({
    enableDragDebug: false,
    debugMode: false
  });
});

app.post('/api/debug/:key', (req, res) => {
  // Simular actualizaciÃ³n de debug
  res.json({ success: true });
});

app.get('/api/chats', (req, res) => {
  res.json({});
});

app.get('/api/chats/:id', (req, res) => {
  res.json({
    chat: { id: req.params.id, createdAt: Date.now(), updatedAt: Date.now(), preview: 'Chat' },
    messages: []
  });
});

app.post('/api/chats/:id', (req, res) => {
  res.json({ success: true });
});

app.delete('/api/chats/:id', (req, res) => {
  res.json({ success: true });
});

app.delete('/api/chats', (req, res) => {
  res.json({ success: true });
});

app.get('/api/app/versions', (req, res) => {
  res.json({
    clippy: '1.0.0',
    electron: undefined,
    nodeLlamaCpp: '3.0.0'
  });
});

app.post('/api/app/check-updates', (req, res) => {
  res.json({ success: true });
});

app.post('/api/view/:view', (req, res) => {
  res.json({ success: true });
});

app.post('/api/models/:name', (req, res) => {
  res.json({ success: true });
});

app.delete('/api/models/:name', (req, res) => {
  res.json({ success: true });
});

// Endpoint de salud
app.get('/api/health', (req, res) => {
  res.json({ 
    status: 'ok', 
    timestamp: new Date().toISOString(),
    model: {
      loaded: !!llama,
      loading: isModelLoading,
      error: modelError
    }
  });
});

// Manejo de errores global
app.use((error, req, res, next) => {
  console.error('Error no manejado:', error);
  res.status(500).json({ error: 'Error interno del servidor' });
});

// Iniciar servidor
app.listen(port, () => {
  console.log(`ğŸš€ Servidor LLM ejecutÃ¡ndose en http://localhost:${port}`);
  console.log(`ğŸ“¡ Endpoint de chat: http://localhost:${port}/api/chat`);
  console.log(`ğŸ” Estado del modelo: http://localhost:${port}/api/model/status`);
  
  // Intentar cargar el modelo al iniciar
  loadModel();
});

// Manejo de cierre graceful
process.on('SIGINT', () => {
  console.log('\nğŸ›‘ Cerrando servidor...');
  process.exit(0);
});

process.on('SIGTERM', () => {
  console.log('\nğŸ›‘ Cerrando servidor...');
  process.exit(0);
});