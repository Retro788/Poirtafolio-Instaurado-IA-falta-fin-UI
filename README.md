# ğŸš€ Portafolio Interactivo con Septiembre.AI

## ğŸ“‹ DescripciÃ³n

Este es un portafolio interactivo que incluye un asistente de IA llamado **Septiembre.AI**, creado por el Ing. Fernando San Gabriel (RetroTheDev). El proyecto combina una interfaz de portafolio moderna con capacidades de chat en tiempo real usando modelos LLM locales.

## ğŸ—ï¸ Arquitectura del Proyecto

### Frontend (React + TypeScript)
- **Puerto**: 3000
- **TecnologÃ­as**: React, TypeScript, CSS personalizado
- **CaracterÃ­sticas**: Interfaz de portafolio, chat con Septiembre.AI, streaming en tiempo real

### Backend LLM (Node.js + Express)
- **Puerto**: 5001
- **TecnologÃ­as**: Node.js, Express, TypeScript, node-llama-cpp
- **CaracterÃ­sticas**: API REST, streaming SSE, gestiÃ³n de modelos LLM

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos

- **Node.js** (versiÃ³n 18 o superior)
- **npm** o **pnpm**
- **Git**
- Al menos **8GB de RAM** para modelos LLM
- **Espacio en disco**: 5-10GB para modelos

### 1. Clonar el Repositorio

```bash
git clone <url-del-repositorio>
cd Portafolio-chatUiSolved-main
```

### 2. Configurar el Backend LLM

```bash
# Navegar al directorio del servidor
cd server

# Instalar dependencias
npm install

# Compilar TypeScript
npm run build

# Iniciar el servidor
npm start
```

El servidor backend estarÃ¡ disponible en: `http://localhost:5001`

### 3. Configurar el Frontend

```bash
# Regresar al directorio raÃ­z
cd ..

# Instalar dependencias
npm install

# Iniciar el servidor de desarrollo
npm start
```

El frontend estarÃ¡ disponible en: `http://localhost:3000`

## ğŸ¤– GestiÃ³n de Modelos LLM

### UbicaciÃ³n de los Modelos

Los modelos LLM se almacenan en:
```
server/storage/downloaded_models/
```

### Modelos Disponibles

El proyecto viene configurado con el modelo **Septiembre Clippy**, que se descarga automÃ¡ticamente desde Hugging Face:

- **Modelo**: `septiembre-clippy`
- **Fuente**: Hugging Face
- **TamaÃ±o**: ~4GB
- **Formato**: GGUF (cuantizado)

### Descargar Modelos

#### MÃ©todo 1: AutomÃ¡tico (Recomendado)

1. Inicia el backend (`npm start` en `/server`)
2. Abre el frontend (`npm start` en raÃ­z)
3. Haz clic en el asistente Septiembre.AI
4. El modelo se descargarÃ¡ automÃ¡ticamente al primer uso

#### MÃ©todo 2: Manual via API

```bash
# Descargar modelo especÃ­fico
curl -X POST http://localhost:5001/api/chat/models/septiembre-clippy/download

# Verificar modelos disponibles
curl http://localhost:5001/api/chat/models
```

### Agregar Nuevos Modelos

Para agregar nuevos modelos, edita el archivo:
```
server/storage/models.json
```

Ejemplo de configuraciÃ³n:
```json
{
  "models": [
    {
      "id": "nuevo-modelo",
      "name": "Nuevo Modelo",
      "description": "DescripciÃ³n del modelo",
      "url": "https://huggingface.co/usuario/modelo/resolve/main/modelo.gguf",
      "size": "4.2GB",
      "type": "chat"
    }
  ]
}
```

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno

Crea un archivo `.env` en la raÃ­z del proyecto:

```env
# Frontend
REACT_APP_API_ROOT=http://localhost:5001
REACT_APP_CHAT_API=http://localhost:5001/api/chat

# Backend (opcional)
PORT=5001
MODELS_DIR=./storage/downloaded_models
```

### ConfiguraciÃ³n del Servidor

Edita `server/index.ts` para ajustar:

- **Puerto del servidor**
- **TamaÃ±o del contexto** del modelo
- **ParÃ¡metros de generaciÃ³n** (temperature, top_k)
- **LÃ­mites de memoria**

### Personalizar Septiembre.AI

El system prompt se puede modificar en:
```
src/components/applications/Clippy/sharedState.ts
```

Cambia la constante `DEFAULT_SYSTEM_PROMPT` para personalizar el comportamiento del asistente.

## ğŸš€ Scripts Disponibles

### Frontend

```bash
npm start          # Iniciar servidor de desarrollo
npm run build      # Compilar para producciÃ³n
npm test           # Ejecutar pruebas
npm run eject      # Exponer configuraciÃ³n de Webpack
```

### Backend

```bash
npm start          # Iniciar servidor (requiere compilaciÃ³n previa)
npm run build      # Compilar TypeScript
npm run dev        # Desarrollo con recarga automÃ¡tica
npm run clean      # Limpiar archivos compilados
```

## ğŸ” Estructura del Proyecto

```
Portafolio-chatUiSolved-main/
â”œâ”€â”€ ğŸ“ public/                    # Archivos estÃ¡ticos
â”œâ”€â”€ ğŸ“ src/                       # CÃ³digo fuente del frontend
â”‚   â”œâ”€â”€ ğŸ“ components/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ applications/
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“ Clippy/        # Componentes de Septiembre.AI
â”‚   â”‚   â”œâ”€â”€ ğŸ“ os/                # Componentes del sistema operativo
â”‚   â”‚   â””â”€â”€ ğŸ“ showcase/          # Componentes del portafolio
â”‚   â”œâ”€â”€ ğŸ“ services/              # Servicios y APIs
â”‚   â””â”€â”€ ğŸ“ assets/                # Recursos (imÃ¡genes, fuentes, etc.)
â”œâ”€â”€ ğŸ“ server/                    # Backend LLM
â”‚   â”œâ”€â”€ ğŸ“ llm/                   # LÃ³gica del chat y LLM
â”‚   â”œâ”€â”€ ğŸ“ models/                # GestiÃ³n de modelos
â”‚   â”œâ”€â”€ ğŸ“ storage/               # Almacenamiento de modelos y datos
â”‚   â”‚   â”œâ”€â”€ models.json           # ConfiguraciÃ³n de modelos
â”‚   â”‚   â””â”€â”€ ğŸ“ downloaded_models/ # Modelos descargados
â”‚   â””â”€â”€ ğŸ“ dist/                  # CÃ³digo compilado
â”œâ”€â”€ ğŸ“„ package.json               # Dependencias del frontend
â”œâ”€â”€ ğŸ“„ .env                       # Variables de entorno
â””â”€â”€ ğŸ“„ README.md                  # Este archivo
```

## ğŸ§ª Pruebas y VerificaciÃ³n

### Verificar Backend

```bash
# Estado del servidor
curl http://localhost:5001/api/health

# Modelos disponibles
curl http://localhost:5001/api/chat/models

# Probar chat
curl -X POST http://localhost:5001/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "modelAlias": "septiembre-clippy",
    "systemPrompt": "Eres Septiembre.AI",
    "initialPrompts": [{"role": "user", "content": "Hola"}],
    "topK": 40,
    "temperature": 0.7
  }'
```

### Verificar Frontend

1. Abre `http://localhost:3000`
2. Haz clic en el asistente Septiembre.AI
3. EnvÃ­a un mensaje de prueba
4. Verifica que la respuesta aparezca en streaming

## ğŸ› SoluciÃ³n de Problemas

### El modelo no se descarga

1. Verifica la conexiÃ³n a internet
2. Revisa los logs del servidor: `server/logs/`
3. Verifica espacio en disco disponible
4. Intenta descarga manual via API

### Error de memoria insuficiente

1. Cierra otras aplicaciones
2. Reduce el `contextSize` en `server/llm/chat-router.ts`
3. Usa un modelo mÃ¡s pequeÃ±o

### El frontend no se conecta al backend

1. Verifica que el backend estÃ© corriendo en puerto 5001
2. Revisa las variables de entorno en `.env`
3. Verifica CORS en el servidor

### Respuestas lentas del modelo

1. Verifica que tienes suficiente RAM
2. Usa un modelo cuantizado mÃ¡s agresivamente
3. Reduce el `contextSize`
4. Considera usar GPU si estÃ¡ disponible

## ğŸ“š DocumentaciÃ³n Adicional

- **[STREAMING_IMPLEMENTATION.md](./STREAMING_IMPLEMENTATION.md)**: Detalles tÃ©cnicos del streaming
- **[README_LLM.md](./README_LLM.md)**: DocumentaciÃ³n especÃ­fica del LLM
- **[server/README.md](./server/README.md)**: DocumentaciÃ³n del backend

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ‘¨â€ğŸ’» Autor

**Ing. Fernando San Gabriel** (RetroTheDev)
- Estudiante de Ing. Sistemas Computacionales (ITSX)
- Estudiante de Ing. MecatrÃ³nica (UANL)
- IBM Champion nominee
- Google Developers Group Lead
- AWS Community Builder
- Fundador de Hexile Technologies y Connectec

---

Â¿Necesitas ayuda? Â¡Abre un issue o contacta al desarrollador! ğŸš€